{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string, os, reportlab, io, random\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from datetime import datetime\n",
    "import matplotlib.cm as cm\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.styles import getSampleStyleSheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy chats saved to Data/chats.txt\n"
     ]
    }
   ],
   "source": [
    "def generate_ai_dummy_chat(num_iterations=100, file_path='Data/chats.txt'):\n",
    "    chat_template = \"\"\"1/03/2024, 1:19 pm - +00 1111111 joined the AI Research Group\n",
    "1/03/2024, 6:15 pm - +00 7774444: How do you think AI will change education? ðŸŽ“ðŸ’¡\n",
    "1/03/2024, 6:18 pm - +00 6665555: Itâ€™s already happening! Personalized AI tutors are making a big difference. My niece uses one for math, and itâ€™s amazing.\n",
    "1/03/2024, 6:22 pm - +00 5556666: Do you think it could replace teachers entirely, though? I feel like human interaction is still essential.\n",
    "1/03/2024, 6:25 pm - +00 7774444: Probably not entirely. Teachers and AI can complement each other, right? Hybrid learning is the way forward.\n",
    "1/03/2024, 6:30 pm - +00 6665555: On a different note, AI music composition is mind-blowing. Have you heard those AI-generated symphonies? ðŸŽµðŸ¤–\n",
    "1/03/2024, 6:33 pm - +00 7774444: Oh yes! Iâ€™ve been trying out Boomy and Amper Music. Theyâ€™re surprisingly creative!\n",
    "2/03/2024, 6:45 pm - +00 5556666: The integration of AI in smart homes is fascinating. ðŸ ðŸ§ \n",
    "2/03/2024, 6:50 pm - +00 4447777: Especially when it comes to energy management. My smart home setup adjusts my AC based on my schedule. Huge energy savings!\n",
    "2/03/2024, 6:55 pm - +00 5556666: Iâ€™ve been thinking of setting up something like that. Any recommendations for good systems?\n",
    "2/03/2024, 7:00 pm - +00 4447777: Nest Thermostat is great. You can integrate it with Google Assistant too.\n",
    "3/03/2024, 7:15 pm - +00 3338888: AI chatbots are getting more human-like every day! ðŸ’¬ðŸ¤–\n",
    "3/03/2024, 7:18 pm - +00 2229999: Yeah, but some are a little too human-like. I got freaked out when a bot remembered something I said days ago. ðŸ˜…\n",
    "3/03/2024, 7:25 pm - +00 1113333: Thatâ€™s probably GPT-4 or something similar. They use memory to keep conversations more coherent. Itâ€™s impressive tech.\n",
    "4/03/2024, 8:15 pm - +00 9995555: AI-generated literature is impressive! ðŸ“šâœ¨\n",
    "4/03/2024, 8:20 pm - +00 8886666: I tried getting ChatGPT to write a story for me. It was good, but it felt a little too formulaic. Still, itâ€™s great for brainstorming ideas.\n",
    "4/03/2024, 8:25 pm - +00 5559999: True, but imagine pairing AI with human writers. The creative possibilities are endless!\n",
    "5/03/2024, 8:45 pm - +00 7777777: AI-powered virtual assistants are a blessing! I use mine to schedule everything from meetings to workout reminders. ðŸ—£ï¸ðŸ’»\n",
    "5/03/2024, 8:50 pm - +00 6668888: Same! Although mine keeps reminding me to stand up every hour. ðŸ˜„ I appreciate it, but I am standing, okay?!\n",
    "6/03/2024, 9:30 pm - +00 4440000: AI's impact on social media is undeniable. ðŸ“±ðŸ¤–\n",
    "6/03/2024, 9:33 pm - +00 3331111: You mean like content recommendations? The algorithms are spot-on but can be a bit addictive.\n",
    "6/03/2024, 9:45 pm - +00 3331111: AI in personalized marketing is enhancing customer experiences too. ðŸ“ŠðŸ›’\n",
    "6/03/2024, 9:50 pm - +00 2222222: Yeah, but it feels invasive when ads know too much about me. Itâ€™s like theyâ€™re reading my mind.\n",
    "7/03/2024, 10:15 pm - +00 0000001: AI can detect diseases earlier than ever! ðŸ¥ðŸ”¬\n",
    "7/03/2024, 10:18 pm - +00 4447777: Itâ€™s revolutionary for rare diseases. A friendâ€™s relative got diagnosed early because of AI imaging analysis.\n",
    "7/03/2024, 10:25 pm - +00 0000001: Exactly! AI is saving lives in ways we never imagined a decade ago.\n",
    "8/03/2024, 10:45 pm - +00 2222223: AI is changing how we work remotely. ðŸŒðŸ’¼\n",
    "8/03/2024, 10:50 pm - +00 3333334: Especially with AI transcription tools like Otter. Note-taking during meetings is so much easier now.\n",
    "9/03/2024, 11:15 pm - +00 4444445: Smart thermostats powered by AI are so energy-efficient. ðŸ”¥ðŸŒ¡ï¸\n",
    "9/03/2024, 11:20 pm - +00 5555556: True, but what about data privacy? Are we trading efficiency for security?\n",
    "12/03/2024, 2:15 pm - ~ Ximena Morales removed +00 00001111 for spamming AI memes\n",
    "12/03/2024, 2:17 pm - +00 3333333: ðŸ˜† Guess someone overdid it.\n",
    "15/03/2024, 2:20 pm - +00 4444444: ðŸ¤” Do you think AI can achieve true creativity?\n",
    "15/03/2024, 2:25 pm - +00 5555556: It can mimic creativity, but it doesnâ€™t understand it the way humans do. Itâ€™s more of a tool for now.\n",
    "19/03/2024, 2:38 pm - +00 9999999: ðŸ˜’ Iâ€™m skeptical about AI-generated content. It lacks soul.\n",
    "19/03/2024, 2:40 pm - +00 00001111: Maybe, but itâ€™s great for drafts. You can always add the â€œsoulâ€ yourself.\n",
    "23/03/2024, 3:15 pm - +00 9998888: Has anyone tried the new AI chatbot update? It feels so real! ðŸ’¬âœ¨\n",
    "23/03/2024, 3:20 pm - +00 8887777: Yeah, I asked it for book recommendations, and it nailed my preferences!\n",
    "25/03/2024, 3:30 pm - +00 8887777: Neuralink's latest breakthrough is mind-blowing! ðŸ§ âš¡\n",
    "25/03/2024, 3:35 pm - +00 7776666: Agreed. Brain-computer interfaces are straight out of science fiction.\n",
    "28/03/2024, 4:15 pm - +00 5554444: Is AI the future of gaming? ðŸ•¹ï¸ðŸ¤”\n",
    "28/03/2024, 4:20 pm - +00 4443333: Totally. NPCs powered by AI could make games so much more immersive. Imagine NPCs that actually learn from your actions.\n",
    "30/03/2024, 4:45 pm - +00 3332222: I just used AI to analyze my stock portfolio. ðŸ“ˆðŸ¤“\n",
    "30/03/2024, 4:50 pm - +00 2221111: Does it actually work? Iâ€™ve been hesitant to trust those tools.\n",
    "31/03/2024, 5:00 pm - +00 2221111: AI in fashion design is revolutionary! ðŸ‘—ðŸ¤–\n",
    "31/03/2024, 5:10 pm - +00 1110000: Definitely. AI-generated patterns are so creative, and they save time for designers.\n",
    "1/04/2024, 5:15 pm - +00 1110000: AI is enhancing our daily lives in so many ways. ðŸ ðŸ’¡\n",
    "2/04/2024, 5:30 pm - +00 0001111: I just read about AI in climate change research. ðŸŒðŸŒ±\n",
    "3/04/2024, 5:45 pm - +00 9992222: AI-driven drones are being used for delivery! ðŸšðŸ“¦\n",
    "4/04/2024, 6:00 pm - +00 8883333: The impact of AI on job markets is immense. ðŸ’¼ðŸ¤–\n",
    "5/04/2024, 6:15 pm - +00 7775555: AI-generated medical imagery is improving diagnostic precision. ðŸ¥ðŸ“Š\n",
    "6/04/2024, 6:30 pm - +00 6664444: AI can predict weather patterns with higher accuracy. ðŸŒ¦ï¸ðŸ”\n",
    "7/04/2024, 6:45 pm - +00 5553333: AI-powered customer feedback analysis is insightful. ðŸ’¡ðŸ›ï¸\n",
    "8/04/2024, 7:00 pm - +00 4442222: The AI in autonomous robots is redefining logistics. ðŸššðŸ¤–\n",
    "9/04/2024, 7:15 pm - +00 3331110: AI language models are excellent for translations. ðŸŒðŸ“–\n",
    "10/04/2024, 7:30 pm - +00 2220000: AI innovations in medicine are saving lives. â¤ï¸ðŸ”¬\n",
    "11/04/2024, 7:45 pm - +00 1112221: AI for wildlife tracking is helping conservation efforts. ðŸ¾ðŸŒ\n",
    "12/04/2024, 8:00 pm - +00 2223332: AI is revolutionizing the entertainment industry! ðŸŽ¥ðŸ¤–\n",
    "13/04/2024, 8:15 pm - +00 3334443: AI-based language tutors are great for learning! ðŸ“šðŸ¤“\n",
    "14/04/2024, 8:30 pm - +00 4445554: AI's role in sustainable energy is promising. ðŸŒžðŸ”‹\n",
    "15/04/2024, 8:45 pm - +00 5556665: AI is reshaping financial analysis. ðŸ’¹ðŸ¤–\n",
    "16/04/2024, 9:00 pm - +00 6667776: AI is reducing food waste in supply chains. ðŸ½ï¸ðŸ› ï¸\n",
    "17/04/2024, 9:15 pm - +00 7778887: AI and blockchain together are a powerful duo. ðŸ”—ðŸ¤–\n",
    "18/04/2024, 9:30 pm - +00 8889998: AI in urban planning is creating smarter cities. ðŸ™ï¸ðŸ“Š\"\"\"\n",
    "\n",
    "    dummy_chats = [chat_template for _ in range(num_iterations)]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Write the dummy chats to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(\"\\n\".join(dummy_chats))\n",
    "    \n",
    "    print(f\"Dummy chats saved to {file_path}\")\n",
    "\n",
    "# Example usage\n",
    "generate_ai_dummy_chat(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Parse Chat\n",
    "def parse_chat(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "    chat_data = []\n",
    "    current_message = None\n",
    "\n",
    "    # Regex to match the timestamped lines\n",
    "    line_pattern = re.compile(\n",
    "        r\"(\\d{1,2}/\\d{1,2}/\\d{4}), (\\d{1,2}:\\d{2}\\s(?:am|pm)) - (.+?): (.+)\"\n",
    "    )\n",
    "    system_message_pattern = re.compile(\n",
    "        r\"(\\d{1,2}/\\d{1,2}/\\d{4}), (\\d{1,2}:\\d{2}\\s(?:am|pm)) - (.+)\"\n",
    "    )\n",
    "\n",
    "    for line in data:\n",
    "        # Match lines with timestamps\n",
    "        match = line_pattern.match(line)\n",
    "        if match:\n",
    "            date, time, sender, message = match.groups()\n",
    "            chat_data.append([date, time, sender, message])\n",
    "        else:\n",
    "            # Match system messages\n",
    "            sys_match = system_message_pattern.match(line)\n",
    "            if sys_match:\n",
    "                date, time, message = sys_match.groups()\n",
    "                chat_data.append([date, time, None, message])  # No sender for system messages\n",
    "            elif chat_data:\n",
    "                # Multiline message: Append to the last message\n",
    "                chat_data[-1][3] += f\" {line.strip()}\"\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(chat_data, columns=['Date', 'Time', 'Sender', 'Message'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess Chat\n",
    "def preprocess_chat(chat_df):\n",
    "    # Filter out system messages (where Sender is None)\n",
    "    chat_df = chat_df[chat_df['Sender'].notnull()].copy()\n",
    "\n",
    "    # Define cleaning logic\n",
    "    def clean_message(message):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = word_tokenize(message.lower())\n",
    "        tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Apply cleaning\n",
    "    chat_df['Cleaned_Message'] = chat_df['Message'].apply(clean_message)\n",
    "    return chat_df\n",
    "\n",
    "def summarize_chat(chat_df, top_n=5):\n",
    "    chat_df['Message_Length'] = chat_df['Message'].apply(len)\n",
    "    return chat_df.sort_values(by='Message_Length', ascending=False).head(top_n)[['Sender', 'Message']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Filter by Criteria\n",
    "def filter_by_criteria(chat_df, start_date=None, end_date=None, start_time=None, end_time=None, keywords=None, topic_words=None):\n",
    "    # Step 1: Filter by Date and Time\n",
    "    chat_df['Datetime'] = pd.to_datetime(chat_df['Date'] + ' ' + chat_df['Time'], format='%d/%m/%Y %I:%M %p')\n",
    "\n",
    "    if start_date:\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        chat_df = chat_df[chat_df['Datetime'] >= start_date]\n",
    "    if end_date:\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "        chat_df = chat_df[chat_df['Datetime'] <= end_date]\n",
    "\n",
    "    if start_time or end_time:\n",
    "        chat_df['Time_Only'] = chat_df['Datetime'].dt.time\n",
    "        if start_time:\n",
    "            start_time = pd.to_datetime(start_time, format='%H:%M').time()\n",
    "            chat_df = chat_df[chat_df['Time_Only'] >= start_time]\n",
    "        if end_time:\n",
    "            end_time = pd.to_datetime(end_time, format='%H:%M').time()\n",
    "            chat_df = chat_df[chat_df['Time_Only'] <= end_time]\n",
    "\n",
    "    # Step 2: Filter by Keywords\n",
    "    if keywords:\n",
    "        keyword_pattern = '|'.join([re.escape(keyword) for keyword in keywords])\n",
    "        chat_df = chat_df[chat_df['Cleaned_Message'].str.contains(keyword_pattern, case=False, na=False)]\n",
    "\n",
    "    # Step 3: Filter by Topic Words\n",
    "    if topic_words:\n",
    "        topic_pattern = '|'.join([re.escape(word) for word in topic_words])\n",
    "        chat_df = chat_df[chat_df['Cleaned_Message'].str.contains(topic_pattern, case=False, na=False)]\n",
    "\n",
    "    return chat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualization - Trends\n",
    "def visualize_trends(chat_df):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    chat_df['Date_Only'] = chat_df['Datetime'].dt.date\n",
    "    message_counts = chat_df.groupby('Date_Only').size()\n",
    "    \n",
    "    # Plot using seaborn\n",
    "    sns.lineplot(data=message_counts, palette='YlGnBu')\n",
    "    plt.title('Message Trends Over Time', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Number of Messages', fontsize=12)\n",
    "    \n",
    "    # Add a colored area under the curve\n",
    "    plt.fill_between(message_counts.index, message_counts.values, color='skyblue', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Report/message_trends_plot.png')\n",
    "    plt.close()\n",
    "    return message_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualization - Keywords\n",
    "def visualize_keywords(keywords):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Use a gradient color palette\n",
    "    colors = sns.color_palette(\"YlGnBu\", len(keywords))\n",
    "    ax = sns.barplot(x=list(keywords.values()), y=list(keywords.keys()), palette=colors)\n",
    "    plt.title('Top Keywords', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('TF-IDF Score', fontsize=12)\n",
    "    plt.ylabel('Keywords', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Report/keywords_plot.png')\n",
    "    plt.close()\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Visualization - Topics\n",
    "def visualize_topics(topics):\n",
    "    topic_labels = [f\"Topic {i+1}\" for i, _ in enumerate(topics)]\n",
    "    topic_words = [', '.join([word.split('*')[1].strip('\"') for word in topic.split(' + ')]) for _, topic in topics]\n",
    "    topic_sizes = [len(words.split(', ')) for words in topic_words]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Use a vivid, distinct color palette\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(topics)))\n",
    "    \n",
    "    plt.pie(\n",
    "        topic_sizes, \n",
    "        labels=[f\"Topic {i+1}\\n{words}\" for i, words in enumerate(topic_words)], \n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=colors,\n",
    "        wedgeprops={'edgecolor': 'white', 'linewidth': 1},\n",
    "        pctdistance=0.85\n",
    "    )\n",
    "    plt.title('Topics Distribution', fontsize=16, fontweight='bold')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Report/topics_plot.png')\n",
    "    plt.close()\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Generate Summary with Filters and Visualization\n",
    "def extract_topics(chat_df, num_topics=3):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    texts = [tokenizer.tokenize(msg.lower()) for msg in chat_df['Cleaned_Message']]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "    # Extract topic words\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    topic_words = []\n",
    "    for topic_num, topic in topics:\n",
    "        words = [word.split('*')[1].strip('\"') for word in topic.split(' + ')]\n",
    "        topic_words.extend(words)\n",
    "    return topics, topic_words\n",
    "\n",
    "def generate_report(chat_df, topics, keywords, summary, message_counts):\n",
    "    # Ensure Report directory exists\n",
    "    os.makedirs('Report', exist_ok=True)\n",
    "    \n",
    "    # PDF Report\n",
    "    pdf_path = 'Report/report.pdf'\n",
    "    doc = SimpleDocTemplate(pdf_path, pagesize=letter)\n",
    "    story = []\n",
    "    \n",
    "    # Styles\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = styles['Title']\n",
    "    heading_style = styles['Heading2']\n",
    "    normal_style = styles['Normal']\n",
    "\n",
    "    # Title\n",
    "    story.append(Paragraph(\"Chat Analysis Report\", title_style))\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    # Summary Statistics\n",
    "    story.append(Paragraph(\"Summary Statistics\", heading_style))\n",
    "    stats_data = [\n",
    "        ['Metric', 'Value'],\n",
    "        ['Total Messages', str(len(chat_df))],\n",
    "        ['Date Range', f\"{chat_df['Datetime'].min().date()} to {chat_df['Datetime'].max().date()}\"],\n",
    "        ['Unique Senders', str(chat_df['Sender'].nunique())],\n",
    "    ]\n",
    "    stats_table = Table(stats_data, colWidths=[200, 200])\n",
    "    stats_table.setStyle(TableStyle([\n",
    "        ('BACKGROUND', (0,0), (-1,0), colors.grey),\n",
    "        ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),\n",
    "        ('ALIGN', (0,0), (-1,-1), 'CENTER'),\n",
    "        ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0,0), (-1,0), 12),\n",
    "        ('BOTTOMPADDING', (0,0), (-1,0), 12),\n",
    "        ('BACKGROUND', (0,1), (-1,-1), colors.beige),\n",
    "        ('GRID', (0,0), (-1,-1), 1, colors.black)\n",
    "    ]))\n",
    "    story.append(stats_table)\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    # Add charts\n",
    "    chart_paths = ['Report/topics_plot.png', 'Report/keywords_plot.png', 'Report/message_trends_plot.png']\n",
    "    for chart_path in chart_paths:\n",
    "        if os.path.exists(chart_path):\n",
    "            img = Image(chart_path, width=400, height=300)\n",
    "            story.append(img)\n",
    "            story.append(Spacer(1, 12))\n",
    "\n",
    "    # Topics Details\n",
    "    story.append(Paragraph(\"Topics Analysis\", heading_style))\n",
    "    for i, (_, topic) in enumerate(topics, 1):\n",
    "        story.append(Paragraph(f\"Topic {i}: {topic}\", normal_style))\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    # Keywords\n",
    "    story.append(Paragraph(\"Top Keywords\", heading_style))\n",
    "    keywords_text = \", \".join([f\"{k} ({v:.2f})\" for k, v in keywords.items()])\n",
    "    story.append(Paragraph(keywords_text, normal_style))\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    # Build PDF\n",
    "    doc.build(story)\n",
    "    print(f\"PDF report generated at {pdf_path}\")\n",
    "\n",
    "def generate_summary(file_path, top_n_keywords=10, top_n_messages=5, num_topics=3, filter_user=None, start_date=None, end_date=None, start_time=None, end_time=None, filter_keywords=None, filter_topics=None):\n",
    "    # Step 1: Parse\n",
    "    chat_df = parse_chat(file_path)\n",
    "\n",
    "    # Step 2: Preprocess\n",
    "    chat_df = preprocess_chat(chat_df)\n",
    "\n",
    "    # Step 3: Apply Filters\n",
    "    chat_df = filter_by_criteria(chat_df, start_date=start_date, end_date=end_date, start_time=start_time, end_time=end_time, keywords=filter_keywords, topic_words=filter_topics)\n",
    "    \n",
    "    if filter_user:\n",
    "        chat_df = chat_df[chat_df['Sender'] == filter_user]\n",
    "        if chat_df.empty:\n",
    "            print(f\"No messages found for user: {filter_user}\")\n",
    "            return\n",
    "\n",
    "    # Step 4: Extract Keywords\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=top_n_keywords)\n",
    "        tfidf_matrix = vectorizer.fit_transform(chat_df['Cleaned_Message'])\n",
    "        keywords = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.sum(axis=0).tolist()[0]))\n",
    "        print(\"Top Keywords:\", keywords)\n",
    "        visualize_keywords(keywords)\n",
    "    except ValueError as e:\n",
    "        print(\"Error during keyword extraction:\", e)\n",
    "        return\n",
    "\n",
    "    # Step 5: Summarize Messages\n",
    "    summary = summarize_chat(chat_df, top_n=top_n_messages)\n",
    "    print(\"\\nTop Messages:\\n\", summary)\n",
    "\n",
    "    # Step 6: Topic Modeling\n",
    "    topics, topic_words = extract_topics(chat_df, num_topics=num_topics)\n",
    "    print(\"\\nTopics:\")\n",
    "    for topic_num, topic in topics:\n",
    "        print(f\"Topic {topic_num}: {topic}\")\n",
    "    topics = visualize_topics(topics)\n",
    "\n",
    "    # Step 7: Visualize Trends\n",
    "    chat_df['Date_Only'] = chat_df['Datetime'].dt.date\n",
    "    message_counts = chat_df.groupby('Date_Only').size()\n",
    "    message_counts = visualize_trends(chat_df)\n",
    "\n",
    "    # Generate PDF Report\n",
    "    generate_report(chat_df, topics, keywords, summary, message_counts)\n",
    "\n",
    "    return topic_words, list(keywords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate initial summary and get topic/keyword suggestions\n",
    "topics, keywords = generate_summary(\n",
    "    'Data/chats.txt',\n",
    "    top_n_keywords=10,\n",
    "    top_n_messages=5,\n",
    "    num_topics=3,\n",
    "    start_date='2024-11-25',\n",
    "    end_date='2024-12-01'\n",
    ")\n",
    "\n",
    "# Let the user select topics and keywords for further filtering\n",
    "selected_topics = ['linkedin', 'https']  # Replace with user-selected topics\n",
    "selected_keywords = ['ai', 'linkedin']  # Replace with user-selected keywords\n",
    "\n",
    "# Filter the chat based on selected topics and keywords\n",
    "generate_summary(\n",
    "    'Data/chats.txt',\n",
    "    filter_topics=selected_topics,\n",
    "    filter_keywords=selected_keywords,\n",
    "    start_date='2024-11-25',\n",
    "    end_date='2024-12-01'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "10. Make screen recording for using this app on PC and mobile and post on linkedin\n",
    "11. Post update of work in group and on status. Anchor github and linkedin links to whatsapp post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
